# -*- coding: utf-8 -*-
"""TravelTide_Segmentation_Steps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18w-D2Gn17-pHr2f0RVYTxjs60UL7KjQO

# TravelTide Segmentation — Structured Colab Notebook

This notebook implements your segmentation pipeline divided into the following steps:

1. **Data connection** (DB or CSV)
2. **Data cleaning**
3. **Data analysis (EDA)**
4. **Feature engineering**
5. **Segmentation (K-Means)**
6. **Outputs & next steps**

*Generated on: 2025-09-03*

## 0) Mount Drive & Project Paths
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Project directories in Drive
from pathlib import Path
import os

PROJECT_ROOT = Path("/content/drive/MyDrive/Project Data").resolve()
DATA_RAW_DIR = PROJECT_ROOT / "data" / "raw"
DATA_PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
OUTPUT_MODELS_DIR = PROJECT_ROOT / "output" / "models"
REPORTS_DIR = PROJECT_ROOT / "reports"
SCRIPTS_DIR = PROJECT_ROOT / "scripts"

for d in [DATA_RAW_DIR, DATA_PROCESSED_DIR, OUTPUT_MODELS_DIR, REPORTS_DIR, SCRIPTS_DIR]:
    d.mkdir(parents=True, exist_ok=True)

print("PROJECT_ROOT:", PROJECT_ROOT)
print("RAW:", DATA_RAW_DIR)
print("PROCESSED:", DATA_PROCESSED_DIR)

"""## 1) Install Dependencies"""

import sys, subprocess
subprocess.check_call([sys.executable, "-m", "pip", "install",
                       "pandas", "SQLAlchemy", "psycopg2-binary",
                       "scikit-learn", "joblib", "python-dateutil"])

"""## 2) Data Connection (choose DB **or** CSV)"""

import pandas as pd
from sqlalchemy import create_engine, text

# Option A — Load from CSVs saved to Drive (recommended if you've already exported raw tables)
def load_from_csv():
    users    = pd.read_csv(DATA_RAW_DIR / "users.csv")
    sessions = pd.read_csv(DATA_RAW_DIR / "sessions.csv")
    flights  = pd.read_csv(DATA_RAW_DIR / "flights.csv")
    hotels   = pd.read_csv(DATA_RAW_DIR / "hotels.csv")
    return users, sessions, flights, hotels

# Option B — Pull directly from Postgres (fill in your DB URL)
DB_URL = ""  # e.g. "postgresql+psycopg2://USER:PASS@HOST/TravelTide?sslmode=require"

def load_from_db(db_url: str):
    if not db_url:
        raise ValueError("Please set DB_URL to connect to Postgres.")
    eng = create_engine(db_url, pool_pre_ping=True)
    with eng.connect() as c:
        users    = pd.read_sql(text("SELECT * FROM users;"), c)
        sessions = pd.read_sql(text("SELECT * FROM sessions;"), c)
        flights  = pd.read_sql(text("SELECT * FROM flights;"), c)
        hotels   = pd.read_sql(text("SELECT * FROM hotels;"), c)
    return users, sessions, flights, hotels

# Choose data source here
SOURCE = "csv"  # "csv" or "db"

if SOURCE == "csv":
    users, sessions, flights, hotels = load_from_csv()
else:
    users, sessions, flights, hotels = load_from_db(DB_URL)

print(users.shape, sessions.shape, flights.shape, hotels.shape)
users.head()

"""## 3) Data Cleaning"""

import numpy as np
import pandas as pd

def to_dt(df, cols):
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_datetime(df[c], errors="coerce")
    return df

def clean_tables(users, sessions, flights, hotels):
    # Parse dates
    users    = to_dt(users,    ["birthdate", "sign_up_date"])
    sessions = to_dt(sessions, ["session_start", "session_end"])
    flights  = to_dt(flights,  ["departure_time", "return_time"])
    hotels   = to_dt(hotels,   ["check_in_time", "check_out_time"])

    # Fix hotel nights if recomputable
    if {"nights","check_in_time","check_out_time"}.issubset(hotels.columns):
        hotels["nights_recomputed"] = (hotels["check_out_time"] - hotels["check_in_time"]).dt.days
        bad = hotels["nights"].isna() | (pd.to_numeric(hotels["nights"], errors="coerce") <= 0)
        use = bad & hotels["nights_recomputed"].gt(0)
        hotels.loc[use, "nights"] = hotels.loc[use, "nights_recomputed"]
        hotels.drop(columns=["nights_recomputed"], inplace=True, errors="ignore")

    # Coerce numerics
    for df, cols in [
        (flights,  ["seats", "checked_bags", "base_fare_usd"]),
        (hotels,   ["nights", "rooms", "hotel_per_room_usd"]),
        (sessions, ["page_clicks", "flight_discount_amount", "hotel_discount_amount"])
    ]:
        for c in cols:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors="coerce")

    # Session duration
    if {"session_start","session_end"}.issubset(sessions.columns):
        sessions["session_minutes"] = (sessions["session_end"] - sessions["session_start"]).dt.total_seconds() / 60.0

    # Drop duplicates on primary keys
    if "user_id" in users.columns:
        users = users.drop_duplicates(subset=["user_id"])
    if "session_id" in sessions.columns:
        sessions = sessions.drop_duplicates(subset=["session_id"])
    if "trip_id" in flights.columns:
        flights = flights.drop_duplicates(subset=["trip_id"])
    if "trip_id" in hotels.columns:
        hotels = hotels.drop_duplicates(subset=["trip_id"])

    return users, sessions, flights, hotels

users, sessions, flights, hotels = clean_tables(users, sessions, flights, hotels)
print("Cleaned shapes:", users.shape, sessions.shape, flights.shape, hotels.shape)

"""## 4) Data Analysis (EDA)"""

# Use matplotlib (no seaborn), one chart per cell, and do not specify colors.
import matplotlib.pyplot as plt

# Example 1: Sessions page_clicks distribution
if "page_clicks" in sessions.columns:
    plt.figure()
    sessions["page_clicks"].dropna().hist(bins=30)
    plt.title("Sessions: Page Clicks Distribution")
    plt.xlabel("page_clicks"); plt.ylabel("count")
    plt.show()

# Example 2: Flights base fare distribution
import matplotlib.pyplot as plt
if "base_fare_usd" in flights.columns:
    plt.figure()
    flights["base_fare_usd"].dropna().hist(bins=30)
    plt.title("Flights: Base Fare (USD) Distribution")
    plt.xlabel("base_fare_usd"); plt.ylabel("count")
    plt.show()

"""## 5) Feature Engineering"""

def winsor(series, up=0.99, lo=0.0):
    s = pd.to_numeric(series, errors="coerce")
    return s.clip(lower=s.quantile(lo), upper=s.quantile(up))

def safe_div(a, b):
    a = pd.to_numeric(a, errors="coerce")
    b = pd.to_numeric(b, errors="coerce")
    with np.errstate(divide="ignore", invalid="ignore"):
        r = a / b
    return r.replace([np.inf, -np.inf], np.nan)

COHORT_MIN_MONTHS = 6
MIN_SESSIONS = 2

def build_user_features(users, sessions, flights, hotels):
    # Determine max data time for recency
    max_dates = []
    for df, cols in [(sessions, ["session_start","session_end"]),
                     (flights,  ["departure_time","return_time"]),
                     (hotels,   ["check_in_time","check_out_time"]),
                     (users,    ["sign_up_date"])]:
        for c in cols:
            if c in df.columns:
                max_dates.append(df[c].max())
    data_max = pd.to_datetime(pd.Series(max_dates)).max()

    # Sessions aggregates
    s = sessions.copy()
    for col in ("flight_booked","hotel_booked","cancellation"):
        if col in s.columns:
            s[col] = pd.to_numeric(s[col], errors="coerce").fillna(0)
        else:
            s[col] = 0

    sess_agg = s.groupby("user_id", as_index=False).agg(
        sessions_total=("session_id","count"),
        sessions_with_cancellation=("cancellation","sum"),
        sessions_with_flight_book=("flight_booked","sum"),
        sessions_with_hotel_book=("hotel_booked","sum"),
        avg_page_clicks=("page_clicks","mean"),
        avg_session_minutes=("session_minutes","mean"),
        last_session_time=("session_end","max")
    )

    # Flights aggregates per trip
    f = flights.copy()
    if "seats" in f.columns:
        f["seats"] = f["seats"].fillna(1)
    if {"base_fare_usd","seats"}.issubset(f.columns):
        f["flight_spend_pre_disc"] = f["base_fare_usd"] * f["seats"]
    f_agg = f.groupby("trip_id", as_index=False).agg(
        flight_spend_pre_disc=("flight_spend_pre_disc","sum"),
        flight_checked_bags=("checked_bags","sum"),
        return_time=("return_time","max")
    )

    # Hotels aggregates per trip
    h = hotels.copy()
    if "rooms" in h.columns:
        h["rooms"] = h["rooms"].fillna(1)
    if "nights" in h.columns:
        h["nights"] = h["nights"].fillna(0)
    if {"hotel_per_room_usd","rooms","nights"}.issubset(h.columns):
        h["hotel_spend_pre_disc"] = h["hotel_per_room_usd"] * h["rooms"] * h["nights"]
    h_agg = h.groupby("trip_id", as_index=False).agg(
        hotel_spend_pre_disc=("hotel_spend_pre_disc","sum"),
        nights_total=("nights","sum"),
        rooms_total=("rooms","sum"),
        check_out_time=("check_out_time","max")
    )

    # Join trip-level to sessions, then roll up to user
    s_trips = s[["session_id","user_id","trip_id"]].copy()
    s_join = s_trips.merge(f_agg, on="trip_id", how="left").merge(h_agg, on="trip_id", how="left")
    spend_user = s_join.groupby("user_id", as_index=False).agg(
        total_flight_spend=("flight_spend_pre_disc","sum"),
        total_hotel_spend=("hotel_spend_pre_disc","sum"),
        total_nights=("nights_total","sum"),
        total_rooms=("rooms_total","sum")
    )

    u = users.copy()
    u = u.merge(sess_agg, on="user_id", how="left").merge(spend_user, on="user_id", how="left")

    # Coerce numerics and fillna
    for c in ["sessions_total","sessions_with_cancellation","sessions_with_flight_book","sessions_with_hotel_book",
              "avg_page_clicks","avg_session_minutes","total_flight_spend","total_hotel_spend","total_nights","total_rooms"]:
        if c in u.columns:
            u[c] = pd.to_numeric(u[c], errors="coerce").fillna(0)

    # RFM-style features
    u["frequency_trips"] = u["sessions_with_flight_book"] + u["sessions_with_hotel_book"]
    u["monetary_total"]  = u["total_flight_spend"] + u["total_hotel_spend"]

    last_by_session = s_trips.merge(f_agg[["trip_id","return_time"]], on="trip_id", how="left")                              .merge(h_agg[["trip_id","check_out_time"]], on="trip_id", how="left")
    last_by_session["last_booking_time"] = last_by_session[["return_time","check_out_time"]].max(axis=1)
    last_book_user = last_by_session.groupby("user_id", as_index=False)["last_booking_time"].max()

    u = u.merge(last_book_user, on="user_id", how="left")
    u["recency_days"] = (data_max - u["last_booking_time"]).dt.days
    u["recency_days"] = u["recency_days"].fillna((data_max - u["last_session_time"]).dt.days)

    u["cancel_rate"] = safe_div(u["sessions_with_cancellation"], u["sessions_total"])
    u["book_rate"]   = safe_div(u["frequency_trips"], u["sessions_total"])

    if "sign_up_date" in u.columns:
        u["months_on_platform"] = ((data_max - pd.to_datetime(u["sign_up_date"])).dt.days / 30.44).round(1)
    else:
        u["months_on_platform"] = np.nan

    # Cohort filter
    mask = (u["sessions_total"] >= MIN_SESSIONS) & (u["months_on_platform"].fillna(np.inf) >= COHORT_MIN_MONTHS)
    u = u.loc[mask].copy()

    # Winsorize heavy tails
    for c in ["avg_page_clicks","avg_session_minutes","monetary_total","total_flight_spend","total_hotel_spend","frequency_trips","recency_days","total_nights"]:
        if c in u.columns:
            u[c] = winsor(u[c], up=0.99)

    feat_cols = [c for c in [
        "recency_days","frequency_trips","monetary_total","cancel_rate","book_rate",
        "avg_page_clicks","avg_session_minutes","total_nights"
    ] if c in u.columns]

    return u, feat_cols

u_features, feat_cols = build_user_features(users, sessions, flights, hotels)
print("Feature columns:", feat_cols)
u_features[feat_cols].describe()

"""## 6) Segmentation (K-Means) and output files"""

# === K-Means segmentation on cleaned features (Drive: "Project Data") ===
from pathlib import Path
import pandas as pd
import numpy as np

# ---- Paths ----
PROJECT_ROOT = Path("/content/drive/MyDrive/Project Data").resolve()
IN_FEAT  = PROJECT_ROOT / "data" / "processed" / "user_features.csv"
OUT_FEAT = PROJECT_ROOT / "data" / "processed" / "user_features_with_segments.csv"
OUT_SEG  = PROJECT_ROOT / "data" / "processed" / "user_segments.csv"
OUT_PROF = PROJECT_ROOT / "reports" / "cluster_profiles.csv"
OUT_KSEL = PROJECT_ROOT / "reports" / "k_selection_fast.csv"

# ---- Load cleaned features ----
u_features = pd.read_csv(IN_FEAT)

# Use the features that exist
feat_cols = [
    "recency_days","frequency_trips","monetary_total","cancel_rate","book_rate",
    "avg_page_clicks","avg_session_minutes","total_nights"
]
feat_cols = [c for c in feat_cols if c in u_features.columns]
if not feat_cols:
    raise ValueError("No expected feature columns found in user_features.csv")
X_df = u_features[feat_cols].fillna(0).copy()

# ---- Scale ----
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X_df.values)

# ---- Fast K selection: CH (full) + sampled silhouette ----
from sklearn.metrics import calinski_harabasz_score, silhouette_score
from sklearn.cluster import MiniBatchKMeans, KMeans

k_range = [3,4,5,6]
results = []
n = X.shape[0]
SAMPLE_SIL = min(8000, n)  # lower if still slow
rng = np.random.default_rng(42)
if SAMPLE_SIL < n:
    sample_idx = rng.choice(n, size=SAMPLE_SIL, replace=False)
    X_sil = X[sample_idx]
else:
    X_sil = X

# pass 1: CH using MiniBatchKMeans (fast)
for k in k_range:
    mbk = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=4096, n_init=3, max_iter=100)
    labels_full = mbk.fit_predict(X)
    ch = calinski_harabasz_score(X, labels_full)
    results.append({"k": k, "ch": ch})

results_df = pd.DataFrame(results).sort_values("ch", ascending=False)
top_k = results_df["k"].head(2).tolist()

# pass 2: silhouette on a sample for the top 2
sil_rows = []
for k in top_k:
    km = KMeans(n_clusters=k, random_state=42, n_init=5, max_iter=150, algorithm="elkan")
    labels_full = km.fit_predict(X)
    labels_sample = labels_full if SAMPLE_SIL == n else labels_full[sample_idx]
    sil = silhouette_score(X_sil, labels_sample) if len(np.unique(labels_sample)) > 1 else -1.0
    sil_rows.append({"k": k, "silhouette_sample": sil})
sil_df = pd.DataFrame(sil_rows)

k_sel = (results_df.merge(sil_df, on="k", how="left")
                   .sort_values(["silhouette_sample","ch"], ascending=[False, False])
                   .iloc[0]["k"])
print("K candidates:\n", results_df.merge(sil_df, on="k", how="left"))
print(f"\nSelected K = {k_sel}")

# ---- Final fit ----
final_km = KMeans(n_clusters=int(k_sel), random_state=42, n_init=8, max_iter=200, algorithm="elkan")
labels = final_km.fit_predict(X)

# ---- Build cluster profile ----
u_tmp = u_features.copy()
u_tmp["cluster"] = labels
profile = (
    u_tmp.groupby("cluster")[feat_cols]
    .mean()
    .reset_index()
)

# ---- Name the segments (heuristic based on centroid patterns) ----
def _name_segments(prof: pd.DataFrame) -> dict:
    p = prof.set_index("cluster")
    # fill missing cols with zeros to avoid key errors
    for c in ["frequency_trips","monetary_total","total_nights","recency_days"]:
        if c not in p.columns: p[c] = 0.0

    # scores (higher = more likely)
    scores = {}
    scores["Frequent Business Travelers"] = (
        p["frequency_trips"].rank(ascending=False) +
        0.6 * p["monetary_total"].rank(ascending=False)
    )
    scores["Luxury Experience Seekers"] = (
        p["monetary_total"].rank(ascending=False) -
        0.2 * p["frequency_trips"].rank(ascending=True)
    )
    scores["Budget-Conscious Explorers"] = (
        p["frequency_trips"].rank(ascending=False) -
        0.5 * p["monetary_total"].rank(ascending=False)
    )
    scores["Family Vacationers"] = (
        p["total_nights"].rank(ascending=False) +
        0.2 * p["frequency_trips"].rank(ascending=False)
    )
    scores["At-Risk / Dormant Users"] = (
        p["recency_days"].rank(ascending=False) +
        0.2 * p["frequency_trips"].rank(ascending=True)
    )

    mapping = {}
    taken = set()
    # assign best remaining cluster for each label
    for name, s in scores.items():
        # choose the top-ranked cluster not already used
        candidate = s.sort_values(ascending=False).index[0]
        # if taken, choose next best
        for cid in s.sort_values(ascending=False).index:
            if cid not in taken:
                candidate = cid
                break
        mapping[candidate] = name
        taken.add(candidate)

    # any leftover clusters get generic names
    for cid in p.index:
        if cid not in mapping:
            mapping[cid] = f"Segment {int(cid)}"
    return mapping

name_map = _name_segments(profile.copy())
profile["segment_name"] = profile["cluster"].map(name_map)

# ---- Save outputs with names ----
u_out = u_features.copy()
u_out["cluster"] = labels
u_out["segment_name"] = u_out["cluster"].map(name_map)

# Ensure folders exist
OUT_PROF.parent.mkdir(parents=True, exist_ok=True)

u_out.to_csv(OUT_FEAT, index=False)
(u_out[["user_id","cluster","segment_name"]]
 if "user_id" in u_out.columns else u_out[["cluster","segment_name"]]
).to_csv(OUT_SEG, index=False)
profile.to_csv(OUT_PROF, index=False)
(results_df.merge(sil_df, on="k", how="left")).to_csv(OUT_KSEL, index=False)

print("\n✅ Wrote:")
print(" -", OUT_FEAT)
print(" -", OUT_SEG)
print(" -", OUT_PROF)
print(" -", OUT_KSEL)